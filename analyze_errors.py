"""
Error Analysis Script
ì˜¤ë‹µì¸ ì˜ˆì¸¡ë“¤ì— ëŒ€í•´ SQL ì‹¤í–‰ ê²°ê³¼ë¥¼ ë³´ì—¬ì£¼ê³  LLMì—ê²Œ reasoningê³¼ classificationì„ ìš”ì²­í•©ë‹ˆë‹¤.
"""

import os
import yaml
import argparse
import json
from typing import List, Dict, Any
from decimal import Decimal
from datetime import datetime, date
import mysql.connector
from mysql.connector import Error
from dotenv import load_dotenv

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

from src.model.openai_model import OpenAIModel
from src.model.gemini_model import GeminiModel
from src.model.deepseek_model import DeepSeekModel


class CustomJSONEncoder(json.JSONEncoder):
    """MySQL ë°ì´í„° íƒ€ì…ì„ JSONìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ì»¤ìŠ¤í…€ ì¸ì½”ë”"""
    def default(self, obj):
        if isinstance(obj, Decimal):
            return float(obj)
        if isinstance(obj, (datetime, date)):
            return obj.isoformat()
        if isinstance(obj, bytes):
            return obj.decode('utf-8', errors='ignore')
        return super().default(obj)


def get_model(model_config: Dict[str, Any]):
    """ì„¤ì •ì— ë”°ë¼ ì ì ˆí•œ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±"""
    provider = model_config['provider']
    
    # OpenAIModel ë“±ì€ ì „ì²´ configë¥¼ ë°›ì•„ì„œ config['model']ë¡œ ì ‘ê·¼
    # ê·¸ë˜ì„œ {'model': model_config} í˜•íƒœë¡œ ì „ë‹¬
    wrapped_config = {'model': model_config}
    
    if provider == 'openai' or provider == 'openai_with_tools':
        return OpenAIModel(wrapped_config)
    elif provider == 'gemini':
        return GeminiModel(wrapped_config)
    elif provider == 'deepseek':
        return DeepSeekModel(wrapped_config)
    else:
        raise ValueError(f"Unsupported model provider: {provider}")


def execute_sql(db_config: Dict[str, Any], sql: str, limit: int = 5) -> Dict[str, Any]:
    """
    SQLì„ ì‹¤í–‰í•˜ê³  ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    
    Returns:
        {
            'success': bool,
            'rows': List[tuple] or None,
            'columns': List[str] or None,
            'error': str or None,
            'row_count': int
        }
    """
    try:
        # ë¹„ë°€ë²ˆí˜¸ í™˜ê²½ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜¤ê¸°
        password = os.getenv('MYSQL_PASSWORD') if db_config.get('password') == 'from_env' else db_config.get('password')
        
        connection = mysql.connector.connect(
            host=db_config['host'],
            port=db_config['port'],
            user=db_config['user'],
            password=password,
            database='dw'  # beaver dw ë°ì´í„°ë² ì´ìŠ¤
        )
        
        cursor = connection.cursor()
        
        # LIMIT ì¶”ê°€ (ì´ë¯¸ ìˆìœ¼ë©´ ì¶”ê°€ ì•ˆí•¨)
        sql_to_execute = sql
        if 'LIMIT' not in sql.upper():
            sql_to_execute = f"{sql.rstrip(';')} LIMIT {limit}"
        
        cursor.execute(sql_to_execute)
        
        rows = cursor.fetchall()
        columns = [desc[0] for desc in cursor.description] if cursor.description else []
        
        # ì „ì²´ row countë¥¼ ìœ„í•´ ì›ë³¸ ì¿¼ë¦¬ ì‹¤í–‰ (COUNTë§Œ)
        count_sql = f"SELECT COUNT(*) FROM ({sql.rstrip(';')}) as count_query"
        cursor.execute(count_sql)
        total_count = cursor.fetchone()[0]
        
        cursor.close()
        connection.close()
        
        return {
            'success': True,
            'rows': rows,
            'columns': columns,
            'error': None,
            'row_count': total_count
        }
        
    except Error as e:
        return {
            'success': False,
            'rows': None,
            'columns': None,
            'error': str(e),
            'row_count': 0
        }


def format_sql_result(result: Dict[str, Any]) -> str:
    """SQL ì‹¤í–‰ ê²°ê³¼ë¥¼ ì½ê¸° ì‰¬ìš´ í…ìŠ¤íŠ¸ë¡œ í¬ë§·íŒ…"""
    if not result['success']:
        return f"âŒ SQL Execution Error: {result['error']}"
    
    output = []
    output.append(f"âœ… Query executed successfully")
    output.append(f"ğŸ“Š Total rows: {result['row_count']}")
    
    if result['rows']:
        output.append(f"\nğŸ” Sample results (first {len(result['rows'])} rows):")
        output.append("Columns: " + " | ".join(result['columns']))
        output.append("-" * 80)
        
        for row in result['rows']:
            output.append(" | ".join(str(val) if val is not None else "NULL" for val in row))
    else:
        output.append("\n(No rows returned)")
    
    return "\n".join(output)


def load_evaluation_results(exec_results_path: str) -> Dict[int, bool]:
    """
    exec_results_detail.jsonì—ì„œ ê° ì˜ˆì¸¡ì˜ ì •ë‹µ ì—¬ë¶€ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
    
    Returns:
        {index: is_correct} ë”•ì…”ë„ˆë¦¬
    """
    if not os.path.exists(exec_results_path):
        raise FileNotFoundError(f"exec_results_detail.json not found at {exec_results_path}")
    
    with open(exec_results_path, 'r', encoding='utf-8') as f:
        exec_results = json.load(f)
    
    # exec_results format: [{'sql_idx': 0, 'res': 1}, ...]
    result_map = {}
    for item in exec_results:
        idx = item['sql_idx']
        is_correct = (item['res'] == 1)
        result_map[idx] = is_correct
    
    return result_map


def execute_and_compare(db_config: Dict, gold_sql: str, predicted_sql: str) -> bool:
    """
    goldì™€ predicted SQLì„ ì‹¤í–‰í•˜ê³  ê²°ê³¼ë¥¼ ë¹„êµí•©ë‹ˆë‹¤.
    
    Returns:
        True if results match (correct prediction), False otherwise
    """
    gold_result = execute_sql(db_config, gold_sql, limit=None)  # ì „ì²´ ì‹¤í–‰
    pred_result = execute_sql(db_config, predicted_sql, limit=None)
    
    # ë‘˜ ë‹¤ ì„±ê³µí–ˆì„ ë•Œë§Œ ë¹„êµ
    if gold_result['success'] and pred_result['success']:
        gold_set = set(tuple(row) for row in gold_result['rows'])
        pred_set = set(tuple(row) for row in pred_result['rows'])
        return gold_set == pred_set
    elif not gold_result['success'] and not pred_result['success']:
        # ë‘˜ ë‹¤ ì—ëŸ¬ë©´... ì¼ë‹¨ í‹€ë¦° ê±¸ë¡œ
        return False
    else:
        # í•˜ë‚˜ë§Œ ì—ëŸ¬ë©´ ëª…ë°±íˆ í‹€ë¦¼
        return False


def create_analysis_prompt(question: str, gold_sql: str, predicted_sql: str, 
                          gold_result: Dict[str, Any], pred_result: Dict[str, Any]) -> str:
    """ì˜¤ë‹µ ë¶„ì„ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
    
    prompt = f"""You are an expert SQL analyst. Analyze why the predicted SQL query is incorrect.

**Question:**
{question}

**Gold (Correct) SQL:**
```sql
{gold_sql}
```

**Gold SQL Execution Result:**
{format_sql_result(gold_result)}

---

**Predicted (Incorrect) SQL:**
```sql
{predicted_sql}
```

**Predicted SQL Execution Result:**
{format_sql_result(pred_result)}

---

**Task:**
Please provide:

1. **Error Reasoning**: Explain in detail WHY the predicted SQL is incorrect. What is the fundamental difference from the gold SQL? What logic error was made?

2. **Error Classification**: Classify the error into ONE of the following categories:
   - **COLUMN_VALUE_ERROR**: The LLM guessed the wrong literal value for a column because it doesn't know the actual data in the database. Examples:
     * Question says "supervisor" but actual DB value is 'Activity leader'
     * Question says "Computer Science" but actual DB value is 'Electrical Eng & Computer Sci'
     * LLM used LIKE '%center for international studies%' when the table itself (CIS_COURSE_CATALOG) already contains only that department's data
     * FLOOR column contains non-numeric values like 'B1', 'G' that LLM didn't anticipate
   - **JOIN_ERROR**: Wrong JOIN type (INNER vs LEFT), missing JOIN, or incorrect JOIN condition
   - **CARDINALITY_ERROR**: M:N relationship not handled properly (missing DISTINCT, GROUP BY, etc.)
   - **FILTER_ERROR**: Wrong WHERE condition or missing/extra filters (but the value itself is correct)
   - **COLUMN_ERROR**: Wrong columns selected or aggregation issues
   - **LOGIC_ERROR**: Fundamental misunderstanding of the question
   - **SYNTAX_ERROR**: SQL syntax error preventing execution
   - **OTHER**: Other types of errors

   **IMPORTANT**: If the predicted SQL would be correct IF the LLM knew the actual column value in the database, classify it as COLUMN_VALUE_ERROR. This is a data knowledge issue, not a SQL logic issue.

3. **Suggested Fix**: Briefly describe what changes are needed to fix the predicted SQL.

Please format your response as JSON:
```json
{{
    "reasoning": "Detailed explanation here...",
    "error_category": "CATEGORY_NAME",
    "suggested_fix": "Brief description of fix..."
}}
```
"""
    
    return prompt


def analyze_errors(predictions_path: str, predict_dw_path: str, config_path: str, 
                  output_path: str, max_samples: int = None):
    """
    ì˜¤ë‹µë“¤ì„ ë¶„ì„í•˜ì—¬ reasoningê³¼ classificationì„ ìƒì„±í•©ë‹ˆë‹¤.
    
    Args:
        predictions_path: predictions.json ê²½ë¡œ
        predict_dw_path: predict_dw.json ê²½ë¡œ (ì •ë‹µ ì—¬ë¶€ í¬í•¨)
        config_path: ì„¤ì • íŒŒì¼ ê²½ë¡œ
        output_path: ë¶„ì„ ê²°ê³¼ ì €ì¥ ê²½ë¡œ
        max_samples: ë¶„ì„í•  ìµœëŒ€ ìƒ˜í”Œ ìˆ˜ (Noneì´ë©´ ì „ì²´)
    """
    
    # ì„¤ì • ë¡œë“œ
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    # Predictions ë¡œë“œ
    with open(predictions_path, 'r', encoding='utf-8') as f:
        predictions = json.load(f)
    
    # exec_results_detail.jsonì—ì„œ ì •ë‹µ ì—¬ë¶€ ë¡œë“œ
    output_dir = os.path.dirname(predict_dw_path)
    exec_results_path = os.path.join(output_dir, 'exec_results_detail.json')
    correctness_map = load_evaluation_results(exec_results_path)
    
    # Gold SQL ë¡œë“œ
    gold_data_path = os.path.join(config['dataset']['path'], 'formatted_data.json')
    with open(gold_data_path, 'r', encoding='utf-8') as f:
        gold_data = json.load(f)
    
    # Questionì„ keyë¡œ í•˜ëŠ” gold SQL ë§µ ìƒì„±
    gold_sql_map = {item['question']: item['sql'] for item in gold_data}
    
    # LLM ëª¨ë¸ ì´ˆê¸°í™”
    model = get_model(config['model'])
    
    # ì˜¤ë‹µë§Œ í•„í„°ë§ (íŒŒì¼ì—ì„œ ì½ìŒ - ë¹ ë¦„!)
    incorrect_predictions = []
    for idx, pred in enumerate(predictions):
        if idx not in correctness_map:
            continue
        if not correctness_map[idx]:  # ì˜¤ë‹µì¸ ê²½ìš°ë§Œ
            incorrect_predictions.append((idx, pred))
    
    print(f"ğŸ“Š Total predictions: {len(predictions)}")
    print(f"âŒ Incorrect predictions: {len(incorrect_predictions)}")
    
    if max_samples:
        incorrect_predictions = incorrect_predictions[:max_samples]
        print(f"ğŸ”¬ Analyzing first {max_samples} incorrect predictions")
    
    # ë¶„ì„ ê²°ê³¼ ì €ì¥
    analysis_results = []
    
    for i, (idx, pred) in enumerate(incorrect_predictions, 1):
        question = pred['question']
        predicted_sql = pred['predicted_sql']
        gold_sql = gold_sql_map.get(question)
        
        if not gold_sql:
            print(f"âš ï¸ [{i}/{len(incorrect_predictions)}] No gold SQL found for question: {question[:50]}...")
            continue
        
        print(f"\n{'='*80}")
        print(f"ğŸ” [{i}/{len(incorrect_predictions)}] Analyzing question #{idx}")
        print(f"Q: {question[:80]}...")
        print(f"Predicted SQL type: {type(predicted_sql)}")
        if predicted_sql.startswith("Error:"):
            print(f"âš ï¸ Predicted SQL is an error message: {predicted_sql}")
        
        # SQL ì‹¤í–‰
        print("\n  ğŸ“Š Executing Gold SQL...")
        gold_result = execute_sql(config['db_connection'], gold_sql)
        print(f"     Gold result: {'âœ… Success' if gold_result['success'] else 'âŒ Error'} - {gold_result['row_count']} rows")
        
        print("\n  ğŸ“Š Executing Predicted SQL...")
        pred_result = execute_sql(config['db_connection'], predicted_sql)
        print(f"     Predicted result: {'âœ… Success' if pred_result['success'] else 'âŒ Error'}")
        if not pred_result['success']:
            print(f"     Error: {pred_result['error'][:100]}...")
        
        # LLMì—ê²Œ ë¶„ì„ ìš”ì²­
        print("\n  ğŸ¤– Requesting LLM analysis...")
        analysis_prompt = create_analysis_prompt(
            question, gold_sql, predicted_sql, gold_result, pred_result
        )
        print(f"     Prompt length: {len(analysis_prompt)} chars")
        
        try:
            llm_response = model.generate(analysis_prompt)
            
            print(f"     LLM response type: {type(llm_response)}")
            print(f"     LLM response preview: {str(llm_response)[:200]}...")
            
            # ChatCompletion ê°ì²´ì¸ ê²½ìš° content ì¶”ì¶œ
            if hasattr(llm_response, 'choices'):
                response_text = llm_response.choices[0].message.content
                print(f"     Extracted from ChatCompletion object")
            elif isinstance(llm_response, str):
                response_text = llm_response
            else:
                response_text = str(llm_response)
            
            # JSON íŒŒì‹± ì‹œë„
            response_text = response_text.strip()
            # ```json ... ``` ì œê±°
            if response_text.startswith('```json'):
                response_text = response_text[7:]
            if response_text.startswith('```'):
                response_text = response_text[3:]
            if response_text.endswith('```'):
                response_text = response_text[:-3]
            response_text = response_text.strip()
            
            analysis = json.loads(response_text)
            
            print(f"  âœ… Category: {analysis['error_category']}")
            
        except json.JSONDecodeError as e:
            print(f"  âš ï¸ Failed to parse LLM response as JSON: {e}")
            print(f"     Response text: {response_text[:500]}...")
            analysis = {
                "reasoning": response_text if 'response_text' in locals() else str(llm_response),
                "error_category": "UNKNOWN",
                "suggested_fix": "N/A"
            }
        except Exception as e:
            print(f"  âŒ Error during analysis: {type(e).__name__}: {str(e)}")
            print(f"     Full traceback available if needed")
            analysis = {
                "reasoning": f"Error: {str(e)}",
                "error_category": "ERROR",
                "suggested_fix": "N/A"
            }
        
        # ê²°ê³¼ ì €ì¥
        analysis_results.append({
            "question": question,
            "gold_sql": gold_sql,
            "predicted_sql": predicted_sql,
            "gold_execution": {
                "success": gold_result['success'],
                "row_count": gold_result['row_count'],
                "sample_rows": gold_result['rows'][:3] if gold_result['rows'] else [],
                "columns": gold_result['columns'],
                "error": gold_result['error']
            },
            "predicted_execution": {
                "success": pred_result['success'],
                "row_count": pred_result['row_count'],
                "sample_rows": pred_result['rows'][:3] if pred_result['rows'] else [],
                "columns": pred_result['columns'],
                "error": pred_result['error']
            },
            "analysis": analysis
        })
    
    # ê²°ê³¼ ì €ì¥
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(analysis_results, f, indent=2, ensure_ascii=False, cls=CustomJSONEncoder)
    
    print(f"\n{'='*80}")
    print(f"âœ… Analysis complete! Results saved to: {output_path}")
    
    # í†µê³„ ì¶œë ¥
    category_counts = {}
    for result in analysis_results:
        category = result['analysis']['error_category']
        category_counts[category] = category_counts.get(category, 0) + 1
    
    print(f"\nğŸ“Š Error Category Distribution:")
    for category, count in sorted(category_counts.items(), key=lambda x: x[1], reverse=True):
        percentage = (count / len(analysis_results)) * 100
        print(f"  {category}: {count} ({percentage:.1f}%)")


def main():
    parser = argparse.ArgumentParser(description="Analyze incorrect SQL predictions using LLM")
    parser.add_argument("--predictions", required=True, help="Path to predictions.json")
    parser.add_argument("--predict_dw", required=True, help="Path to predict_dw.json")
    parser.add_argument("--config", required=True, help="Path to config YAML file")
    parser.add_argument("--output", required=True, help="Path to save analysis results")
    parser.add_argument("--max_samples", type=int, default=None, 
                       help="Maximum number of samples to analyze (default: all)")
    
    args = parser.parse_args()
    
    analyze_errors(
        predictions_path=args.predictions,
        predict_dw_path=args.predict_dw,
        config_path=args.config,
        output_path=args.output,
        max_samples=args.max_samples
    )


if __name__ == "__main__":
    main()
