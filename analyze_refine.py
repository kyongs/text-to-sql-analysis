"""
Refine Analysis Script
error_analysis.jsonì˜ SQL ë³€ì²œì‚¬ë¥¼ ë¶„ì„í•˜ê³ , LLMìœ¼ë¡œ ê°œì„  ì—¬ë¶€ë¥¼ íŒë‹¨í•©ë‹ˆë‹¤.
"""

import os
import yaml
import argparse
import json
import csv
from typing import List, Dict, Any
from decimal import Decimal
from datetime import datetime, date
from dotenv import load_dotenv
from pathlib import Path

# Load environment variables from .env file
env_path = Path(__file__).parent / '.env'
load_dotenv(dotenv_path=env_path)

from src.model.openai_model import OpenAIModel
from src.model.gemini_model import GeminiModel
from src.model.deepseek_model import DeepSeekModel


class CustomJSONEncoder(json.JSONEncoder):
    """MySQL ë°ì´í„° íƒ€ì…ì„ JSONìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ì»¤ìŠ¤í…€ ì¸ì½”ë”"""
    def default(self, obj):
        if isinstance(obj, Decimal):
            return float(obj)
        if isinstance(obj, (datetime, date)):
            return obj.isoformat()
        if isinstance(obj, bytes):
            return obj.decode('utf-8', errors='ignore')
        return super().default(obj)


def get_model(model_config: Dict[str, Any]):
    """ì„¤ì •ì— ë”°ë¼ ì ì ˆí•œ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±"""
    provider = model_config['provider']
    wrapped_config = {'model': model_config}

    if provider == 'openai' or provider == 'openai_with_tools':
        return OpenAIModel(wrapped_config)
    elif provider == 'gemini' or provider == 'google':
        return GeminiModel(wrapped_config)
    elif provider == 'deepseek':
        return DeepSeekModel(wrapped_config)
    else:
        raise ValueError(f"Unsupported model provider: {provider}")


def load_gold_data(dataset_path: str) -> Dict[int, Dict[str, str]]:
    """
    formatted_data.jsonì—ì„œ NLQì™€ gold SQL ë¡œë“œ
    Returns: {idx: {"question": ..., "gold_sql": ...}}
    """
    gold_data_path = os.path.join(dataset_path, 'formatted_data.json')
    with open(gold_data_path, 'r', encoding='utf-8') as f:
        gold_data = json.load(f)

    return {i: {"question": item['question'], "gold_sql": item['sql']}
            for i, item in enumerate(gold_data)}


def create_analysis_prompt(question: str, gold_sql: str, sql_evolution: List[Dict],
                          show_gold: bool = True) -> str:
    """SQL ë³€ì²œì‚¬ ë¶„ì„ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ ìƒì„±"""

    # SQL ë³€ì²œì‚¬ ë¬¸ìì—´ ìƒì„±
    evolution_str = ""
    for i, item in enumerate(sql_evolution):
        iter_num = item.get('iter', i + 1)
        sql = item.get('sql', '')
        result = item.get('result', 'unknown')
        refine_feedback = item.get('refine_feedback', '')

        evolution_str += f"\n### Iteration {iter_num}\n"
        evolution_str += f"**Result:** {result}\n"
        evolution_str += f"```sql\n{sql}\n```\n"

        if refine_feedback:
            evolution_str += f"\n**Refine Feedback (trigger for next iteration):**\n{refine_feedback}\n"

    if show_gold:
        prompt = f"""You are an expert SQL analyst. Analyze the SQL query evolution and determine if the refinements improved or degraded the query.

**Natural Language Question:**
{question}

**Gold (Correct) SQL:**
```sql
{gold_sql}
```

---

**SQL Evolution (from initial to final):**
{evolution_str}

---

**Task:**
Please analyze:

1. **Evolution Assessment**: Did the SQL improve, stay the same, or get worse across iterations? Compare each iteration to the gold SQL.

2. **Best Iteration**: Which iteration produced the SQL closest to the gold SQL? (iter_1, iter_2, etc.)

3. **Error Analysis**: If the final SQL is still incorrect, explain why. Classify the error:
   - COLUMN_VALUE_ERROR: Wrong literal value (LLM doesn't know actual DB data)
   - JOIN_ERROR: Wrong JOIN type or condition
   - CARDINALITY_ERROR: M:N relationship issues (missing DISTINCT, GROUP BY)
   - FILTER_ERROR: Wrong WHERE condition
   - COLUMN_ERROR: Wrong columns selected
   - LOGIC_ERROR: Fundamental misunderstanding of the question
   - SYNTAX_ERROR: SQL syntax error
   - REFINE_REGRESSION: Refinement made it worse
   - OTHER: Other issues

4. **Refinement Quality**: Was the refine feedback helpful? Did the model respond correctly to it?

Please format your response as JSON:
```json
{{
    "evolution_assessment": "improved/same/worse",
    "best_iteration": "iter_X",
    "final_correct": true/false,
    "error_category": "CATEGORY_NAME or null if correct",
    "reasoning": "í•œêµ­ì–´ë¡œ ìƒì„¸íˆ ì„¤ëª…í•´ì£¼ì„¸ìš” (Explain in Korean)",
    "refinement_quality": "helpful/unhelpful/counterproductive",
    "suggested_fix": "Brief description if still incorrect, or null"
}}
```

**IMPORTANT: The "reasoning" field MUST be written in Korean (í•œêµ­ì–´).**
"""
    else:
        # --no_gold ëª¨ë“œ: gold SQL ì—†ì´ ë¶„ì„
        prompt = f"""You are an expert SQL analyst. Analyze the SQL query evolution and determine if the refinements improved the query.

**Natural Language Question:**
{question}

---

**SQL Evolution (from initial to final):**
{evolution_str}

---

**Task:**
Please analyze WITHOUT seeing the gold SQL:

1. **Evolution Assessment**: Based on the question and the SQL changes, did the refinements seem to improve the query logic?

2. **Query Quality**: Does the final SQL look like it would correctly answer the question?

3. **Potential Issues**: What potential issues do you see in the final SQL?

4. **Refinement Response**: Did the model respond correctly to the refine feedback?

Please format your response as JSON:
```json
{{
    "evolution_assessment": "improved/same/worse",
    "query_looks_correct": true/false,
    "confidence": "high/medium/low",
    "potential_issues": ["issue1", "issue2"],
    "reasoning": "í•œêµ­ì–´ë¡œ ìƒì„¸íˆ ì„¤ëª…í•´ì£¼ì„¸ìš” (Explain in Korean)",
    "refinement_quality": "helpful/unhelpful/counterproductive"
}}
```

**IMPORTANT: The "reasoning" field MUST be written in Korean (í•œêµ­ì–´).**
"""

    return prompt


def analyze_refine_evolution(error_analysis_path: str, config_path: str,
                            output_path: str, no_gold: bool = False,
                            max_samples: int = None):
    """
    error_analysis.jsonì˜ SQL ë³€ì²œì‚¬ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.
    """

    # ì„¤ì • ë¡œë“œ
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)

    # error_analysis.json ë¡œë“œ
    with open(error_analysis_path, 'r', encoding='utf-8') as f:
        error_analysis = json.load(f)

    # Gold data ë¡œë“œ (no_goldê°€ ì•„ë‹ ë•Œë§Œ)
    gold_data = {}
    if not no_gold:
        gold_data = load_gold_data(config['dataset']['path'])

    # LLM ëª¨ë¸ ì´ˆê¸°í™”
    model = get_model(config['model'])

    # iterë³„ ë°ì´í„°ë¥¼ idx ê¸°ì¤€ìœ¼ë¡œ ê·¸ë£¹í•‘
    idx_evolution = {}  # {idx: [iter_1_data, iter_2_data, ...]}

    for iter_key, items in error_analysis.items():
        if not iter_key.startswith('iter_'):
            continue
        iter_num = int(iter_key.split('_')[1])

        for item in items:
            idx = item.get('idx')
            if idx is None:
                continue

            if idx not in idx_evolution:
                idx_evolution[idx] = []

            idx_evolution[idx].append({
                'iter': iter_num,
                'sql': item.get('sql', ''),
                'result': item.get('result', ''),
                'res': item.get('res', ''),
                'refine_feedback': item.get('refine_feedback', '')
            })

    # iter ìˆœì„œëŒ€ë¡œ ì •ë ¬
    for idx in idx_evolution:
        idx_evolution[idx].sort(key=lambda x: x['iter'])

    # 2ê°œ ì´ìƒì˜ iterationì´ ìˆëŠ” ê²ƒë§Œ ë¶„ì„ (refineì´ ë°œìƒí•œ ê²½ìš°)
    items_to_analyze = [(idx, evol) for idx, evol in idx_evolution.items()
                        if len(evol) >= 2]

    print(f"ğŸ“Š Total items with evolution: {len(items_to_analyze)}")

    if max_samples:
        items_to_analyze = items_to_analyze[:max_samples]
        print(f"ğŸ”¬ Analyzing first {max_samples} items")

    # ë¶„ì„ ê²°ê³¼ ì €ì¥
    analysis_results = []

    for i, (idx, evolution) in enumerate(items_to_analyze, 1):
        # Gold dataì—ì„œ NLQì™€ gold SQL ê°€ì ¸ì˜¤ê¸°
        if no_gold:
            question = f"[Question not shown - idx {idx}]"
            gold_sql = None
        else:
            if idx in gold_data:
                question = gold_data[idx]['question']
                gold_sql = gold_data[idx]['gold_sql']
            else:
                print(f"âš ï¸ [{i}/{len(items_to_analyze)}] No gold data for idx {idx}")
                continue

        print(f"\n{'='*80}")
        print(f"ğŸ” [{i}/{len(items_to_analyze)}] Analyzing idx #{idx}")
        print(f"   Iterations: {len(evolution)}")

        # ê° iterationì˜ ê²°ê³¼ ì¶œë ¥
        for ev in evolution:
            result_str = ev.get('result', 'unknown')
            if result_str == '':
                result_str = 'not evaluated'
            print(f"   - iter_{ev['iter']}: {result_str}")

        # LLM ë¶„ì„ ìš”ì²­
        print(f"\n  ğŸ¤– Requesting LLM analysis...")
        analysis_prompt = create_analysis_prompt(
            question, gold_sql, evolution, show_gold=not no_gold
        )

        try:
            llm_response = model.generate(analysis_prompt)

            # ChatCompletion ê°ì²´ì¸ ê²½ìš° content ì¶”ì¶œ
            if hasattr(llm_response, 'choices'):
                response_text = llm_response.choices[0].message.content
            elif isinstance(llm_response, str):
                response_text = llm_response
            else:
                response_text = str(llm_response)

            # ì‘ë‹µì´ ë¹„ì–´ìˆëŠ” ê²½ìš° ì²˜ë¦¬
            if not response_text or response_text.strip() == "":
                print(f"  âš ï¸ Empty response from LLM")
                analysis = {
                    "reasoning": "Empty response from LLM",
                    "evolution_assessment": "unknown",
                    "error_category": "EMPTY_RESPONSE"
                }
            else:
                # JSON íŒŒì‹±
                response_text = response_text.strip()

                # JSON ë¸”ë¡ ì¶”ì¶œ (```json ... ``` ë˜ëŠ” ``` ... ```)
                import re
                json_match = re.search(r'```(?:json)?\s*([\s\S]*?)\s*```', response_text)
                if json_match:
                    response_text = json_match.group(1).strip()

                # ê·¸ë˜ë„ JSONì´ ì•„ë‹ˆë©´ { } ì‚¬ì´ë§Œ ì¶”ì¶œ
                if not response_text.startswith('{'):
                    brace_match = re.search(r'\{[\s\S]*\}', response_text)
                    if brace_match:
                        response_text = brace_match.group(0)

                analysis = json.loads(response_text)

            print(f"  âœ… Assessment: {analysis.get('evolution_assessment', 'unknown')}")
            if 'best_iteration' in analysis:
                print(f"     Best iteration: {analysis['best_iteration']}")
            if 'error_category' in analysis and analysis['error_category']:
                print(f"     Error category: {analysis['error_category']}")

        except json.JSONDecodeError as e:
            print(f"  âš ï¸ Failed to parse LLM response as JSON: {e}")
            analysis = {
                "reasoning": response_text if 'response_text' in locals() else "Parse error",
                "evolution_assessment": "unknown",
                "error_category": "PARSE_ERROR"
            }
        except Exception as e:
            print(f"  âŒ Error during analysis: {type(e).__name__}: {str(e)}")
            analysis = {
                "reasoning": f"Error: {str(e)}",
                "evolution_assessment": "error",
                "error_category": "ANALYSIS_ERROR"
            }

        # ê²°ê³¼ ì €ì¥
        result_item = {
            "idx": idx,
            "question": question if not no_gold else None,
            "gold_sql": gold_sql,
            "sql_evolution": evolution,
            "analysis": analysis
        }
        analysis_results.append(result_item)

    # ê²°ê³¼ ì €ì¥
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(analysis_results, f, indent=2, ensure_ascii=False, cls=CustomJSONEncoder)

    print(f"\n{'='*80}")
    print(f"âœ… Analysis complete! Results saved to: {output_path}")

    # í†µê³„ ì¶œë ¥
    if analysis_results:
        print(f"\nğŸ“Š Summary Statistics:")

        # Evolution assessment í†µê³„
        assessment_counts = {}
        for result in analysis_results:
            assessment = result['analysis'].get('evolution_assessment', 'unknown')
            assessment_counts[assessment] = assessment_counts.get(assessment, 0) + 1

        print(f"\n  Evolution Assessment:")
        for assessment, count in sorted(assessment_counts.items(), key=lambda x: x[1], reverse=True):
            pct = (count / len(analysis_results)) * 100
            print(f"    {assessment}: {count} ({pct:.1f}%)")

        # Error category í†µê³„ (no_goldê°€ ì•„ë‹ ë•Œë§Œ)
        if not no_gold:
            category_counts = {}
            for result in analysis_results:
                category = result['analysis'].get('error_category')
                if category:
                    category_counts[category] = category_counts.get(category, 0) + 1

            if category_counts:
                print(f"\n  Error Categories:")
                for category, count in sorted(category_counts.items(), key=lambda x: x[1], reverse=True):
                    pct = (count / len(analysis_results)) * 100
                    print(f"    {category}: {count} ({pct:.1f}%)")

        # Refinement quality í†µê³„
        quality_counts = {}
        for result in analysis_results:
            quality = result['analysis'].get('refinement_quality', 'unknown')
            quality_counts[quality] = quality_counts.get(quality, 0) + 1

        print(f"\n  Refinement Quality:")
        for quality, count in sorted(quality_counts.items(), key=lambda x: x[1], reverse=True):
            pct = (count / len(analysis_results)) * 100
            print(f"    {quality}: {count} ({pct:.1f}%)")

    # CSV íŒŒì¼ ìƒì„±
    csv_output_path = output_path.replace('.json', '.csv')
    with open(csv_output_path, 'w', encoding='utf-8-sig', newline='') as f:
        writer = csv.writer(f)
        # í—¤ë” ì‘ì„±
        writer.writerow(['idx', 'question', 'gold_sql', 'evolution_assessment', 'error_category', 'reasoning'])

        # ë°ì´í„° ì‘ì„±
        for result in analysis_results:
            idx = result.get('idx', '')
            question = result.get('question', '') or ''
            gold_sql = result.get('gold_sql', '') or ''
            analysis = result.get('analysis', {})
            evolution_assessment = analysis.get('evolution_assessment', '')
            error_category = analysis.get('error_category', '') or ''
            reasoning = analysis.get('reasoning', '')

            writer.writerow([idx, question, gold_sql, evolution_assessment, error_category, reasoning])

    print(f"ğŸ“Š CSV file saved to: {csv_output_path}")


def main():
    parser = argparse.ArgumentParser(description="Analyze SQL refinement evolution")
    parser.add_argument("--error_analysis", required=True,
                       help="Path to error_analysis.json")
    parser.add_argument("--config", required=True,
                       help="Path to config YAML file")
    parser.add_argument("--output", required=True,
                       help="Path to save analysis results")
    parser.add_argument("--no_gold", action='store_true',
                       help="Don't show gold SQL in analysis (only NLQ)")
    parser.add_argument("--max_samples", type=int, default=None,
                       help="Maximum number of samples to analyze")

    args = parser.parse_args()

    analyze_refine_evolution(
        error_analysis_path=args.error_analysis,
        config_path=args.config,
        output_path=args.output,
        no_gold=args.no_gold,
        max_samples=args.max_samples
    )


if __name__ == "__main__":
    main()
